dag_id: etl_user_data
description: "ETL pipeline for user data processing"
schedule: "@daily"
start_date: "2024-01-01"
tags: ["etl", "user-data", "daily"]

# Template inheritance
template:
  extends: "base_etl"
  overrides:
    retries: 3
    retry_delay: 300

# Environment-specific settings
environments:
  dev:
    schedule: "@hourly"
    max_active_runs: 1
    email: ["dev-team@company.com"]
  staging:
    schedule: "@daily"
    max_active_runs: 2
    email: ["staging-team@company.com"]
  prod:
    schedule: "@daily"
    max_active_runs: 1
    email: ["ops@company.com", "data-team@company.com"]

# Default arguments
default_args:
  owner: "data-team"
  depends_on_past: false
  email_on_failure: true
  email_on_retry: false

# Task definitions
tasks:
  - task_id: extract_user_data
    operator: python
    parameters:
      python_callable: "functions.etl.extract_data"
      op_kwargs:
        source_config:
          type: "postgres"
          connection_id: "postgres_default"
          table: "users"
          incremental_column: "updated_at"
    
  - task_id: validate_user_data
    operator: python
    parameters:
      python_callable: "functions.data_validation.validate_data_completeness"
      op_kwargs:
        completeness_config:
          min_records: 100
          max_null_percentage: 5.0
    depends_on: [extract_user_data]
    
  - task_id: transform_user_data
    operator: python
    parameters:
      python_callable: "functions.etl.transform_data"
      op_kwargs:
        transformation_config:
          rules:
            - "standardize_phone_numbers"
            - "validate_email_format"
            - "normalize_addresses"
    depends_on: [validate_user_data]
    
  - task_id: load_user_data
    operator: python
    parameters:
      python_callable: "functions.etl.load_data"
      op_kwargs:
        destination_config:
          type: "redshift"
          connection_id: "redshift_default"
          table: "analytics.users"
          write_disposition: "WRITE_TRUNCATE"
    depends_on: [transform_user_data]
    
  - task_id: validate_etl_results
    operator: python
    parameters:
      python_callable: "functions.etl.validate_etl_results"
      op_kwargs:
        validation_config:
          min_records: 100
          data_quality_checks: true
    depends_on: [load_user_data]

# Task groups
task_groups:
  data_validation:
    tasks: [validate_user_data, validate_etl_results]
    
# Assets for cross-DAG dependencies
assets:
  produces:
    - "dataset://analytics/users"
  consumes: []

# Notifications
notifications:
  on_success:
    - type: "email"
      recipients: ["data-team@company.com"]
  on_failure:
    - type: "email"
      recipients: ["ops@company.com"]
    - type: "slack"
      channel: "#data-alerts"
